{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e876ab4",
   "metadata": {},
   "source": [
    "# Split data into train, test and valdiation sets\n",
    "\n",
    "We need to have the same datasets for training, testing and validation for all models. Thus, we'll do all preprocessing here and save the preprocessed datasets in their own directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c195a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "# DEFINE EXPERIMENT NAME\n",
    "\"\"\"\n",
    "_wordbias experiments involve all features related to tone, polarity, pronoums use etc from gdelt\n",
    "_otherbias experiments involve article, word count and IMGorEMBED features only\n",
    "_allbias experiments involve both of the above features\n",
    "\n",
    "So, we run RFE for each of these separately.\n",
    "\n",
    "For MBFC, do we really want to do these analyses twice, both for with and without categorical data?\n",
    "--> Can't hurt.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "experiment_name = \"_mbfc_allbias_extrafeatures\"\n",
    "\n",
    "# say if it's Robertson/MBFC data\n",
    "is_robertson = False\n",
    "is_mbfc = True\n",
    "\n",
    "# define theme list\n",
    "theme_list = [\n",
    "        # GDELT features -- wordbias experiments\n",
    "        'THEMES_SUBSET', 'PosScore','NegScore','Polarity', 'ActRefDens', 'SelfRefDens',\n",
    "        \n",
    "        # get the eMFD features -- wordbias experiments\n",
    "        #'care-harm','fairness-cheating', 'loyalty-betrayal', 'authority-subversion','sanctity-degradation',\n",
    "        \n",
    "        # MBFC variables\n",
    "        \"Factuality\",\"PressFreedom\",\"MediaType\",\"Traffic\",\"Credibility\",\"Longitude\",\"Latitude\",\"tot_art\"\n",
    "        \n",
    "        # article counts etc -- otherbias experiments\n",
    "        \"article_count\",\"word_count\",\"IMGorEMBED\",\n",
    "        # target variable\n",
    "        \"lean\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a60e2b",
   "metadata": {},
   "source": [
    "## Get Dataset with Known Outlets & Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6ed710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if is_robertson:\n",
    "    # get each batch that we made before, add it to list and then concat into final gdelt file\n",
    "    data_parts = []\n",
    "    for file in tqdm(os.listdir(\"GDELT_GKG/extras/\")):\n",
    "        if file.startswith(\"robertson_outlets_part\"):\n",
    "            data_p = pd.read_csv(\"GDELT_GKG/extras/{}\".format(file))\n",
    "            data_parts.append(data_p)\n",
    "\n",
    "    data = pd.concat(data_parts)\n",
    "\n",
    "    # we also have two outlet columns and the original score, let's remove these\n",
    "    data.drop([\"outlet.1\"],axis=1,inplace=True)\n",
    "\n",
    "    print(data.head())\n",
    "    \n",
    "    # looking at the data, we forgot to add the score from Robertson, let's do that now\n",
    "    robertson = pd.read_csv(\"GDELT_GKG/extras/Bias Ratings/robertson.csv\",usecols=[\"domain\",\"score\"])\n",
    "    robertson = robertson.rename(columns={\"domain\":\"outlet\"})\n",
    "    # let's recode robertson's scores\n",
    "    robertson[\"lean\"] = np.where( # if score lower than -0.6, it's \"left\"--> 0\n",
    "                                robertson[\"score\"] <= -0.6,0, \n",
    "                                # if score between -0.6 and -0.2, it's left lean --> 1\n",
    "                                np.where((robertson[\"score\"] > -0.6) & (robertson[\"score\"] <= -0.2),1,\n",
    "                                np.where((robertson[\"score\"] > -0.2) & (robertson[\"score\"] <=  0.2),2,\n",
    "                                np.where((robertson[\"score\"] > 0.2) & (robertson[\"score\"] < 0.6),3,\n",
    "                                np.where((robertson[\"score\"] >= 0.6),4,-1)))))\n",
    "\n",
    "    # combine the score from robertson to our data\n",
    "    data = data.merge(robertson, on=\"outlet\", how='left')\n",
    "\n",
    "    # we also have two outlet columns and the original score, let's remove these\n",
    "    data.drop([\"score\"],axis=1,inplace=True)\n",
    "\n",
    "    data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08cd749",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" if using 3-way classification\n",
    "# looking at the data, we forgot to add the score from Robertson, let's do that now\n",
    "robertson = pd.read_csv(\"GDELT_GKG/extras/Bias Ratings/robertson.csv\",usecols=[\"domain\",\"score\"])\n",
    "robertson = robertson.rename(columns={\"domain\":\"outlet\"})\n",
    "# let's recode robertson's scores to either 0:\"left\",1:\"center\" or 2:\"right\"\n",
    "robertson[\"lean\"] = np.where(robertson[\"score\"] <= -0.33,0, \n",
    "                                  np.where(robertson[\"score\"]>=0.33,2,1))\n",
    "\n",
    "# combine the score from robertson to our data\n",
    "data = data.merge(robertson, on=\"outlet\", how='left')\n",
    "\n",
    "# we also have two outlet columns and the original score, let's remove these\n",
    "data.drop([\"outlet.1\",\"score\"],axis=1,inplace=True)\n",
    "\n",
    "data.head()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad86a994",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_mbfc:\n",
    "    data = pd.read_csv(\"GDELT_GKG/extras/mbfc_outlet_sentiments.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d39cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57110b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_index(\"outlet\", inplace=True)\n",
    "\n",
    "print(data.shape)\n",
    "# the Press Freedom column has some Nan's, which causes errors for the SVC, let's drop those\n",
    "data = data.dropna() \n",
    "print(data.shape)\n",
    "\n",
    "# extra - already removed outlets with less than 2 articles per day (aka, less than 730 in total)\n",
    "data = data.loc[data[\"tot_art\"] >= 100,:] # let's remove outlets with very few articles\n",
    "print(data.shape) # how many rows do we have now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66c36bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d56c810",
   "metadata": {},
   "source": [
    "## Split into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519f5364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note which columns are which type\n",
    "label_col = \"lean\"\n",
    "categor_cols = [\"Factuality\",\"PressFreedom\",\"MediaType\",\"Traffic\",\"Credibility\"]\n",
    "# we ignore lean, long and lat, as we don't want to scale any of these\n",
    "numeric_cols = data.columns[~data.columns.isin(categor_cols + [label_col])] # \"Longitude\",\"Latitude\" also if MBFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc3620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up dataset, first we do train_test splits\n",
    "# make train and (validation+test) datasets\n",
    "train, val = train_test_split(\n",
    "        data,\n",
    "        test_size=0.3, random_state=42,\n",
    "        # Here we've stratified by lean, can do other variable\n",
    "        stratify=data[\"lean\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8dbf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make validation and test datasets\n",
    "val, test = train_test_split(\n",
    "        val,\n",
    "        test_size=0.5, random_state=42,\n",
    "        stratify=val[\"lean\"]\n",
    "    )\n",
    "\n",
    "print('Train set shape: ', train.shape)\n",
    "print('Validation set shape: ', val.shape)\n",
    "print('Test set shape: ', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cd6150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init and fit the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(train[numeric_cols])\n",
    "\n",
    "# scale numeric columns, (long & lat not included)!\n",
    "train[numeric_cols] = scaler.transform(\n",
    "                            train[numeric_cols]).astype(np.float32)\n",
    "val[numeric_cols] = scaler.transform(\n",
    "                            val[numeric_cols]).astype(np.float32)\n",
    "test[numeric_cols] = scaler.transform(\n",
    "                            test[numeric_cols]).astype(np.float32)\n",
    "\n",
    "# set categorical columns to int (if categorical columns are present, such as in MBFC)\n",
    "train[categor_cols] = train[categor_cols].astype(np.int8)\n",
    "val[categor_cols] = val[categor_cols].astype(np.int8)\n",
    "test[categor_cols] = test[categor_cols].astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1239b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save scaler in case we need it later again\n",
    "joblib.dump(scaler, 'GDELT_GKG/extras/Supporting Files/MinMaxScaler{}.save'.format(experiment_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab338c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85863c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make copy in case something goes wrong in the next few parts while testing\n",
    "train_copy = train.copy(deep=True)\n",
    "val_copy = val.copy(deep=True)\n",
    "test_copy = test.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195c043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset train & test sets in case something has indeed gone wrong\n",
    "train = train_copy.copy(deep=True)\n",
    "val = val_copy.copy(deep=True)\n",
    "test = test_copy.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd5b177",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape, val.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dd73a6",
   "metadata": {},
   "source": [
    "## Alternative Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e922e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_manual_features(X, theme_list):\n",
    "    \"\"\"\n",
    "    Given list of strings of relevance (this can be \n",
    "    either themes or features), return DF with columns \n",
    "    containing these strings.\n",
    "    \"\"\"\n",
    "    selected_cols = [col for col in X.columns for theme in theme_list if theme in col]\n",
    "    return X.loc[:,selected_cols].copy(deep=True)\n",
    "\n",
    "def remove_high_corr_features(X):\n",
    "    \"\"\"\n",
    "    Given dataframe, remove features which correlate\n",
    "    more than 0.95 with another feature. Returns\n",
    "    list of columns to be dropped, and saves this list.\n",
    "    \"\"\"\n",
    "    global experiment_name\n",
    "    # Create correlation matrix\n",
    "    corr_matrix = X.corr(numeric_only=True).abs()\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), \n",
    "                                              k=1).astype(bool))\n",
    "    # Find features with correlation greater than 0.95\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "    # make sure that the target variable can't be accidentally deleted\n",
    "    if \"lean\" in to_drop:\n",
    "        to_drop.remove(\"lean\")\n",
    "    \n",
    "    # save list of features to drop\n",
    "    pd.Series(to_drop).to_csv(\"GDELT_GKG/extras/Supporting Files/high_corr_features_to_drop{}.csv\".format(experiment_name))\n",
    "    \n",
    "    return to_drop\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "def run_feature_elimination(data):\n",
    "    \"\"\"\n",
    "    Information from:\n",
    "    -  https://machinelearningmastery.com/rfe-feature-selection-in-python/\n",
    "    -  https://scikit-learn.org/stable/modules/feature_selection.html\n",
    "\n",
    "    Using a Tree Classifier, we use the feature importances that model yields to determine\n",
    "    which features should be dropped.\n",
    "\n",
    "    This can be done with Cross Validation, as we don't have a particular number of features we need,\n",
    "    but want to determine the optimal number of features automatically.\n",
    "    \n",
    "    Returns a bool mask of columns that were selected.\n",
    "    \"\"\"\n",
    "    global experiment_name\n",
    "    X = data.drop(\"lean\",axis=1)\n",
    "    y = data[\"lean\"]\n",
    "    selector = RFECV(estimator=RandomForestClassifier(random_state=42),\n",
    "                     step=100, cv=10, \n",
    "                     min_features_to_select=10, \n",
    "                     verbose=0)\n",
    "    print(\"fitting, this will take a while...\")\n",
    "    selector = selector.fit(X, y)\n",
    "    print(\"finished fitting!\")\n",
    "    #print(selector.ranking_) # ranking of 1 denotes features that RFE determined as best.\n",
    "    # in each rank, there is [step] number of features (in this case 100)\n",
    "    # save the columns chosen by RFE\n",
    "    pd.Series(selector.support_).to_csv(\"GDELT_GKG/extras/Supporting Files/RFE_selected_features{}.csv\".format(experiment_name))\n",
    "    return selector.support_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7519f4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_MBFC = False\n",
    "manual_features = True\n",
    "remove_high_corr = True\n",
    "feature_elimination = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ada4d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if exclude_MBFC == True:\n",
    "    # also exclude long and lat, as those come from MBFC data\n",
    "    categor_cols.extend([\"Longitude\",\"Latitude\"])\n",
    "    train = train.drop(columns=categor_cols,axis=1,errors=\"ignore\") # ignore errors about column not existing\n",
    "    val = val.drop(columns=categor_cols,axis=1,errors=\"ignore\")\n",
    "    test = test.drop(columns=categor_cols,axis=1,errors=\"ignore\")\n",
    "    \n",
    "    print(train.shape, val.shape, test.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0241cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "if manual_features == True:\n",
    "    # theme list defined at start\n",
    "    train = select_manual_features(train, theme_list=theme_list)\n",
    "    val = select_manual_features(val, theme_list=theme_list)\n",
    "    test = select_manual_features(test, theme_list=theme_list)\n",
    "    \n",
    "    print(train.shape, val.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa5898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if remove_high_corr == True:\n",
    "    to_drop = remove_high_corr_features(train)\n",
    "    # Drop features - 588 columns, Polarity and ActRefDens often corr with Pos/Neg\n",
    "    train.drop(to_drop, axis=1, inplace=True)\n",
    "    val.drop(to_drop, axis=1, inplace=True)\n",
    "    test.drop(to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    print(train.shape, val.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4012128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if feature_elimination == True:\n",
    "    selected_features = run_feature_elimination(train)\n",
    "    # we remove lean, subset to selected columns and add lean back in\n",
    "    y_train = train[\"lean\"]\n",
    "    train = train.drop(\"lean\",axis=1).loc[:,selected_features]\n",
    "    train[\"lean\"] = y_train\n",
    "    # validation set\n",
    "    y_val = val[\"lean\"]\n",
    "    val = val.drop(\"lean\",axis=1).loc[:,selected_features]\n",
    "    val[\"lean\"] = y_val\n",
    "    # test set\n",
    "    y_test = test[\"lean\"]\n",
    "    test = test.drop(\"lean\",axis=1).loc[:,selected_features]\n",
    "    test[\"lean\"] = y_test\n",
    "    print(train.shape, val.shape, test.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680efbb3",
   "metadata": {},
   "source": [
    "## Save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c4d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_dict = {\"exclude_MBFC\":exclude_MBFC,\n",
    "                   \"manual_features\": manual_features,\n",
    "                   \"feature_elimination\":feature_elimination,\n",
    "                   \"remove_high_corr\":remove_high_corr}\n",
    "# save which conditions were used for making this set\n",
    "extra_save_string = \"\"\n",
    "\n",
    "for name, cond in conditions_dict.items():\n",
    "    if cond:\n",
    "        extra_save_string += \"_\" + name\n",
    "\n",
    "print(extra_save_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a123b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the datasets in csv files\n",
    "train.to_csv(\"GDELT_GKG/data/train{}.csv\".format(experiment_name))\n",
    "val.to_csv(\"GDELT_GKG/data/val{}.csv\".format(experiment_name))\n",
    "test.to_csv(\"GDELT_GKG/data/test{}.csv\".format(experiment_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fffe123",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb691c8",
   "metadata": {},
   "source": [
    "# GDELT Datasets\n",
    "\n",
    "The GDELT outlets that have no ground truth from MBFC also need to be preprocessed so they can be predicted on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7c94ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# get each batch that we made before, add it to list and then concat into final gdelt file\n",
    "gdelt_parts = []\n",
    "for file in tqdm(os.listdir(\"GDELT_GKG/extras/\")):\n",
    "    if file.startswith(\"gdelt_outlets_2_part\"):\n",
    "        gdelt_p = pd.read_csv(\"GDELT_GKG/extras/{}\".format(file))\n",
    "        gdelt_parts.append(gdelt_p)\n",
    "        \n",
    "gdelt = pd.concat(gdelt_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a205c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gdelt = pd.read_csv(\"GDELT_GKG/extras/gdelt_outlets_part_00.csv\")\n",
    "gdelt.set_index(\"outlet\", inplace=True)\n",
    "\n",
    "print(gdelt.shape)\n",
    "# let's drop those with nans\n",
    "gdelt = gdelt.dropna()\n",
    "print(gdelt.shape)\n",
    "\n",
    "# extra - already removed outlets with less than 2 articles per day (aka, less than 730 in total)\n",
    "gdelt = gdelt.loc[gdelt[\"tot_art\"] >= 100,:] # let's remove outlets with very few articles\n",
    "print(gdelt.shape) # how many rows do we have now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05f4a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdelt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45107f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale columns - let's first make sure the columns are in the same order - need to take original columns from data instead of train, as train ahs been modified\n",
    "gdelt = gdelt.loc[:,data[numeric_cols].columns]\n",
    "# scale it now - do it on the loc to make sure we get the pandas DF format back\n",
    "gdelt.loc[:,gdelt.columns] = scaler.transform(gdelt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03436249",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_features = True\n",
    "remove_high_corr = True\n",
    "feature_elimination = True\n",
    "exclude_MBFC = False # not needed, as they're not available anyway"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a44514d",
   "metadata": {},
   "source": [
    "### Alternative Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cce4fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87121e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if manual_features == True:\n",
    "    \"\"\" get theme list from before\n",
    "    theme_list = [\n",
    "        # manual themes if wished\n",
    "        #\"SLFID_MILITARY_SPENDING\",\"POLICE\",\"LGBT\",\"IMMIGRATION\",\"ECON_COST_OF_LIVING\",\"MOVEMENT_ENVIRONMENTAL\",\"UNEMPLOYMENT\",\n",
    "        # MBFC variables\n",
    "        #\"Factuality\",\"PressFreedom\",\"MediaType\",\"Traffic\",\"Credibility\",\"Longitude\",\"Latitude\",\"tot_art\"\n",
    "        # target variable\n",
    "        \"lean\",\n",
    "        # only article counts\n",
    "        \"article_count\",\"WordCount\",\"IMGorEMBED\"\n",
    "    ]\n",
    "    \"\"\"\n",
    "    gdelt_subset = select_manual_features(gdelt, theme_list=theme_list)\n",
    "    \n",
    "    print(gdelt_subset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ebdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET to_drop FROM RELEVANT EXPERIMENT\n",
    "to_drop = pd.read_csv(\"GDELT_GKG/extras/Supporting Files/high_corr_features_to_drop{}.csv\".format(experiment_name),index_col=False)\n",
    "to_drop = to_drop[\"0\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ded45b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if remove_high_corr == True:\n",
    "    # Drop features already picked for dropping with train & test sets - 588 columns, Polarity and ActRefDens often corr with Pos/Neg\n",
    "    gdelt_subset.drop(to_drop, axis=1, inplace=True)\n",
    "    print(gdelt_subset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9199820a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET selected_features FROM RELEVANT EXPERIMENT\n",
    "selected_features = pd.read_csv(\"GDELT_GKG/extras/Supporting Files/RFE_selected_features{}.csv\".format(experiment_name))\n",
    "selected_features = selected_features.set_index(\"Unnamed: 0\").T.iloc[0].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88749903",
   "metadata": {},
   "outputs": [],
   "source": [
    "if feature_elimination == True:\n",
    "    # get the featrues we selected before\n",
    "    gdelt_subset = gdelt_subset.loc[:,selected_features]\n",
    "    print(gdelt_subset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7304674",
   "metadata": {},
   "source": [
    "### Save File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfacf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions_dict = {\"exclude_MBFC\":exclude_MBFC,\n",
    "                   \"manual_features\": manual_features,\n",
    "                   \"feature_elimination\":feature_elimination,\n",
    "                   \"remove_high_corr\":remove_high_corr\n",
    "                   }\n",
    "# save which conditions were used for making this set\n",
    "extra_save_string = \"\"\n",
    "\n",
    "for name, cond in conditions_dict.items():\n",
    "    if cond:\n",
    "        extra_save_string += \"_\" + name\n",
    "\n",
    "print(extra_save_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aecfed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389f8be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataset in csv files\n",
    "gdelt_subset.to_csv(\"GDELT_GKG/data/gdelt{}.csv\".format(experiment_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96685e9",
   "metadata": {},
   "source": [
    "# Add Categorical variables to MBCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae6e7124",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"GDELT_GKG/data/train_mbfc_allbias.csv\")\n",
    "train.set_index(\"outlet\",inplace=True)\n",
    "\n",
    "val = pd.read_csv(\"GDELT_GKG/data/val_mbfc_allbias.csv\")\n",
    "val.set_index(\"outlet\",inplace=True)\n",
    "\n",
    "test = pd.read_csv(\"GDELT_GKG/data/test_mbfc_allbias.csv\")\n",
    "test.set_index(\"outlet\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "865811e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Factuality</th>\n",
       "      <th>PressFreedom</th>\n",
       "      <th>MediaType</th>\n",
       "      <th>Traffic</th>\n",
       "      <th>Credibility</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latutude</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outlet</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9news.com</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-100.445882</td>\n",
       "      <td>39.78373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nbc11news.com</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-100.445882</td>\n",
       "      <td>39.78373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12news.com</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-100.445882</td>\n",
       "      <td>39.78373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wibw.com</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-100.445882</td>\n",
       "      <td>39.78373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wifr.com</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-100.445882</td>\n",
       "      <td>39.78373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Factuality  PressFreedom  MediaType  Traffic  Credibility  \\\n",
       "outlet                                                                     \n",
       "9news.com               4           2.0          6        2            2   \n",
       "nbc11news.com           4           2.0          6        1            2   \n",
       "12news.com              4           2.0          6        1            2   \n",
       "wibw.com                4           2.0          6        1            2   \n",
       "wifr.com                4           2.0          6        1            2   \n",
       "\n",
       "                Longitude  Latutude  \n",
       "outlet                               \n",
       "9news.com     -100.445882  39.78373  \n",
       "nbc11news.com -100.445882  39.78373  \n",
       "12news.com    -100.445882  39.78373  \n",
       "wibw.com      -100.445882  39.78373  \n",
       "wifr.com      -100.445882  39.78373  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbfc = pd.read_csv(\"GDELT_GKG/extras/Bias Ratings/MBFC_features.csv\")\n",
    "mbfc.rename({\"URL\":\"outlet\"},axis=1,inplace=True)\n",
    "mbfc.drop(\"Bias Rating\", axis=1,inplace=True)\n",
    "mbfc.set_index(\"outlet\",inplace=True)\n",
    "mbfc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e444d3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_categ = pd.merge(train,mbfc,how=\"left\",on=\"outlet\")\n",
    "val_categ = pd.merge(val,mbfc,how=\"left\",on=\"outlet\")\n",
    "test_categ = pd.merge(test,mbfc,how=\"left\",on=\"outlet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2db4850",
   "metadata": {},
   "outputs": [],
   "source": [
    "categor_cols = [\"Factuality\",\"PressFreedom\",\"MediaType\",\"Traffic\",\"Credibility\"]\n",
    "# set categorical columns to int (if categorical columns are present, such as in MBFC)\n",
    "train_categ[categor_cols] = train_categ[categor_cols].astype(np.int8)\n",
    "val_categ[categor_cols] = val_categ[categor_cols].astype(np.int8)\n",
    "test_categ[categor_cols] = test_categ[categor_cols].astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "faadf958",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save the datasets in csv files\n",
    "train_categ.to_csv(\"GDELT_GKG/data/train{}.csv\".format(experiment_name))\n",
    "val_categ.to_csv(\"GDELT_GKG/data/val{}.csv\".format(experiment_name))\n",
    "test_categ.to_csv(\"GDELT_GKG/data/test{}.csv\".format(experiment_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
