{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d72cadd6",
   "metadata": {},
   "source": [
    "# PyTorch Classification Model\n",
    "\n",
    "https://stackabuse.com/introduction-to-pytorch-for-classification/\n",
    "https://jovian.ml/aakanksha-ns/shelter-outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "507d5cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pytorch_lightning\n",
    "#!pip install lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ec9fec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as torch_optim\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader,Dataset,ConcatDataset\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler,OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score,classification_report,\n",
    "                            confusion_matrix, ConfusionMatrixDisplay,\n",
    "                            roc_auc_score)\n",
    "from time import time\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecb9799",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00ca2d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutletBiasDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for PyTorch's, allowing us to work with the csv dataset.\n",
    "    \n",
    "    From the docs:\n",
    "    All datasets that represent a map from keys to data samples should subclass it. \n",
    "    All subclasses should overwrite __getitem__(), supporting fetching a data sample \n",
    "    for a given key. Subclasses could also optionally overwrite __len__(), which is \n",
    "    expected to return the size of the dataset by many Sampler implementations and \n",
    "    the default options of DataLoader.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, data, numer_cols, categ_cols, label_col):\n",
    "        data = data.copy()\n",
    "        #numerical columns\n",
    "        self.X_num = data.loc[:,numer_cols].copy().values.astype(np.float32)\n",
    "        #categorical columns\n",
    "        self.X_cat = data.loc[:,categ_cols].copy().values.astype(np.int64)\n",
    "        self.y = data[label_col]\n",
    "        self.index = data.index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.index[idx], self.X_num[idx], self.X_cat[idx], self.y[idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc41d55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataModule(pl.LightningDataModule):\n",
    "    \"\"\"\n",
    "    The Pytorch DataModule enables reusing classes, while incorporating\n",
    "    all preprocessing steps: making datasets into OutletBiasDataset class instances.\n",
    "    The DataLoaders are also made here, as required by PyTorch and PyTorch Lightning\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, datadir, dataset_extension,\n",
    "                 num_cols, cat_cols, label_col, num_workers=2,\n",
    "                 batch_size_train=16, batch_size_val=16, batch_size_test=16):\n",
    "        \n",
    "        super().__init__()\n",
    "        os.chdir(datadir)\n",
    "        # datasets\n",
    "        self.train = pd.read_csv(\"train{}.csv\".format(dataset_extension))\n",
    "        self.train.set_index(\"outlet\", inplace=True)\n",
    "        \n",
    "        self.val = pd.read_csv(\"val{}.csv\".format(dataset_extension))\n",
    "        self.val.set_index(\"outlet\", inplace=True)\n",
    "        \n",
    "        self.test = pd.read_csv(\"test{}.csv\".format(dataset_extension))\n",
    "        self.test.set_index(\"outlet\", inplace=True)\n",
    "        \n",
    "        self.full = pd.concat([self.train,self.val,self.test])\n",
    "        \n",
    "        # define other variables\n",
    "        self.num_cols = num_cols\n",
    "        self.cat_cols = cat_cols\n",
    "        self.label_col = label_col\n",
    "        self.num_workers = num_workers\n",
    "        self.batch_size_train = batch_size_train\n",
    "        self.batch_size_val = batch_size_val\n",
    "        self.batch_size_test = batch_size_test\n",
    "\n",
    "    def setup(self, stage=\"\"):\n",
    "        # check dataset shapes\n",
    "        print('Train set shape: ', self.train.shape)\n",
    "        print('Validation set shape: ', self.val.shape)\n",
    "        print('Test set shape: ', self.test.shape)\n",
    "        \n",
    "        # MAKE INTO DATASET FORMAT\n",
    "        self.dataset_train = OutletBiasDataset(self.train,\n",
    "                                               self.num_cols,self.cat_cols, self.label_col)\n",
    "        self.dataset_val = OutletBiasDataset(self.val,\n",
    "                                               self.num_cols,self.cat_cols, self.label_col)\n",
    "        self.dataset_test = OutletBiasDataset(self.test,\n",
    "                                               self.num_cols,self.cat_cols, self.label_col)\n",
    "        self.dataset_full = OutletBiasDataset(self.full,\n",
    "                                       self.num_cols,self.cat_cols, self.label_col)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset_train,\n",
    "            num_workers=self.num_workers,\n",
    "            batch_size=self.batch_size_train,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset_val,\n",
    "            num_workers=self.num_workers,\n",
    "            batch_size=self.batch_size_val,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset_test,\n",
    "            num_workers=self.num_workers,\n",
    "            batch_size=self.batch_size_test,\n",
    "            shuffle=False\n",
    "        )\n",
    "    \n",
    "    def full_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset_full,\n",
    "            num_workers=self.num_workers,\n",
    "            batch_size=self.batch_size_train,\n",
    "            shuffle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8f1abf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularNetModel(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Model class in PyTorch Lightning. Model structure is defined in self.layers().\n",
    "    \n",
    "    Initialize with:\n",
    "    - num_cols: columns in dataset that are numeric,\n",
    "    - cat_cols: columns in dataset that are categorical (and thus will be embedded),\n",
    "    - embedding_size_dict: predetermined embedding sizes per categorical feature,\n",
    "    - n_classes: number of classes to be classified,\n",
    "    - learning_rate: model's learning rate,\n",
    "    - neurons_per_layer_list: list of neurons to be put per layer, for any number of layers,\n",
    "    - dropout_p: dropout probability between each layer\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, num_cols, cat_cols, embedding_size_dict, n_classes,\n",
    "                 neurons_per_layer_list=[512,256,128],\n",
    "                 learning_rate=0.001, dropout_p=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # pytorch lightning black magic, all the arguments can now be\n",
    "        # accessed through self.hparams.[argument]\n",
    "        self.save_hyperparameters()\n",
    "        self.num_cols = num_cols\n",
    "        self.num_len = len(num_cols)\n",
    "        self.cat_cols = cat_cols\n",
    "        self.cat_len = len(cat_cols)\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.embeddings, total_embedding_dim = self._create_embedding_layers(\n",
    "            cat_cols, embedding_size_dict)\n",
    "        \n",
    "        # concatenate the numerical variables and the embedding layers\n",
    "        # then proceed with the rest of the sequential flow\n",
    "        in_features = self.num_len + total_embedding_dim\n",
    "        \n",
    "        # let's finally define the model architecture itself:\n",
    "        all_layers = []\n",
    "        for i in neurons_per_layer_list:\n",
    "            all_layers.append(nn.Linear(in_features, i))\n",
    "            all_layers.append(nn.ReLU(inplace=True))\n",
    "            all_layers.append(nn.BatchNorm1d(i))\n",
    "            all_layers.append(nn.Dropout(dropout_p))\n",
    "            in_features = i\n",
    "            \n",
    "        # add final output layer,\n",
    "        all_layers.append(nn.Linear(neurons_per_layer_list[-1], n_classes))\n",
    "        #Â and put into Sequantial.\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "        \n",
    "        # something to keep track of accuracy with...\n",
    "        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "        self.test_accuracy = Accuracy(task=\"multiclass\", num_classes=self.n_classes)\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_embedding_layers(cat_cols, embedding_size_dict):\n",
    "        \"\"\"construct the embedding layer, 1 per each categorical variable\"\"\"\n",
    "        total_embedding_dim = 0 # keep track of the embed dim for input size\n",
    "        embeddings = {}\n",
    "        for col in cat_cols:\n",
    "            # get embedding size and dim (\"size\" here is the num of classes)\n",
    "            embedding_size = embedding_size_dict[col][0]\n",
    "            embedding_dim = embedding_size_dict[col][1] #Â dim is vector size\n",
    "            total_embedding_dim += embedding_dim\n",
    "            # we add 2 to the output size due to some PyTorch shenanigans, see:\n",
    "            # https://discuss.pytorch.org/t/solved-assertion-srcindex-srcselectdimsize-failed-on-gpu-for-torch-cat/1804/13\n",
    "            embeddings[col] = nn.Embedding(embedding_size+2, embedding_dim)\n",
    "\n",
    "        return nn.ModuleDict(embeddings), total_embedding_dim\n",
    "\n",
    "    def forward(self, num_tensor, cat_tensor):\n",
    "        # run through all the categorical variables through its\n",
    "        # own embedding layer and concatenate them together\n",
    "        cat_outputs = []\n",
    "        for i, col in enumerate(self.hparams.cat_cols):\n",
    "            embedding = self.embeddings[col]\n",
    "            cat_input = cat_tensor[:, i].long() #!\n",
    "            cat_output = embedding(cat_input)\n",
    "            cat_outputs.append(cat_output)\n",
    "\n",
    "        # concatenate to torch tensor with dim=1\n",
    "        cat_outputs = torch.cat(cat_outputs, dim=1)\n",
    "        # concatenate the categorical embedding and numerical layer\n",
    "        all_outputs = torch.cat((num_tensor, cat_outputs), dim=1)\n",
    "        \n",
    "        # now we get the final outputs through the layers we made earlier\n",
    "        final_outputs = self.layers(all_outputs).squeeze(dim=1)\n",
    "        \n",
    "        return F.log_softmax(final_outputs, dim=1)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), \n",
    "                                lr=self.hparams.learning_rate)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        index, num_tensor, cat_tensor, label_tensor = batch\n",
    "        output_tensor = self(num_tensor, cat_tensor)\n",
    "        # compute loss\n",
    "        loss = F.nll_loss(output_tensor, label_tensor)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        index, num_tensor, cat_tensor, label_tensor = batch\n",
    "        output_tensor = self(num_tensor, cat_tensor)\n",
    "        # compute loss\n",
    "        loss = F.nll_loss(output_tensor, label_tensor)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        # compute accuracy\n",
    "        preds = torch.argmax(output_tensor, dim=1)\n",
    "        self.val_accuracy.update(preds, label_tensor)\n",
    "        self.log(\"val_acc\", self.val_accuracy, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        index, num_tensor, cat_tensor, label_tensor = batch\n",
    "        output_tensor = self(num_tensor, cat_tensor)\n",
    "        # compute loss\n",
    "        loss = F.nll_loss(output_tensor, label_tensor)\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        # compute accuracy\n",
    "        preds = torch.argmax(output_tensor, dim=1)\n",
    "        self.val_accuracy.update(preds, label_tensor)\n",
    "        self.log(\"test_acc\", self.val_accuracy, prog_bar=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a732c426",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc64915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# IMPORT DATA FOR MAKING CUSTOM EMBEDDINGS\n",
    "#------------------------------------------------------------------------------\n",
    "# make list of class names in correct order\n",
    "global class_names\n",
    "class_names = [\"left\",\"left center\",\"least biased\",\"right center\",\"right\"] #[\"left\",\"center\",\"right\"]# in order from MBFC mappings\n",
    "\n",
    "dataset_extension = \"_mbfc_allbias_extrafeatures\"\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# GET CATEGORICAL FEATURES TO EMBED AND DETERMINE EMBEDDING SIZES\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# we need to make sure we subset the data from which we get the embeddings to \n",
    "# the columns present in training set\n",
    "train = pd.read_csv(\"GDELT_GKG/data/train{}.csv\".format(dataset_extension))\n",
    "train.set_index(\"outlet\", inplace=True)\n",
    "train_cols = train.columns\n",
    "\n",
    "# get and subset data\n",
    "data = pd.read_csv(\"GDELT_GKG/extras/mbfc_outlet_sentiments.csv\")\n",
    "data.set_index(\"outlet\", inplace=True)\n",
    "data.rename({\"Latitude\":\"Latutude\"},axis=1,inplace=True) # wrongly named column, replace\n",
    "data = data[train_cols]\n",
    "\n",
    "# get categorical and numerical columns\n",
    "label_col = \"lean\"\n",
    "cat_cols = [\"Factuality\",\"PressFreedom\",\"MediaType\",\"Traffic\",\"Credibility\"]\n",
    "num_cols = data.columns[~data.columns.isin(cat_cols + [label_col])]\n",
    "feature_cols = data.columns[~data.columns.isin([label_col])]\n",
    "\n",
    "# for easier processing, let's temporarily set cat column types to \"category\"\n",
    "data[cat_cols] = data[cat_cols].astype('category')\n",
    "# get number of classes per categorical column\n",
    "classes_per_col = {name: len(col.cat.categories) \n",
    "                   for name,col in  data[cat_cols].items()}\n",
    "# determine embedding sizes: embedding size rule from fastai and: http://ethen8181.github.io/machine-learning/deep_learning/tabular/tabular.html\n",
    "embedding_sizes = {col_name: (n_categories, \n",
    "                              min(600, round(1.6 * n_categories ** 0.56))\n",
    "                             )\n",
    "                   for col_name,n_categories in classes_per_col.items()}\n",
    "# another source: https://ai.stackexchange.com/questions/28564/how-to-determine-the-embedding-size\n",
    "\n",
    "# visualize\n",
    "print(embedding_sizes)\n",
    "\n",
    "del classes_per_col, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01f1740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# MAKE DATASET MODULE\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "tabular_data_module = TabularDataModule(\"GDELT_GKG/data\", \n",
    "                                        dataset_extension=dataset_extension,\n",
    "                                        num_cols=num_cols, cat_cols=cat_cols, label_col=label_col, \n",
    "                                        num_workers=2,\n",
    "                                        batch_size_train=32, \n",
    "                                        batch_size_val=32, \n",
    "                                        batch_size_test=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b19daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# MAKE MODEL(S)\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "# we can print out the network architecture for inspection\n",
    "tabular_model = TabularNetModel(num_cols, cat_cols, embedding_sizes, \n",
    "                               n_classes=len(class_names),neurons_per_layer_list = [512,256], dropout_p=0.5)\n",
    "# for viewing architecture:\n",
    "tabular_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa07c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "# MODEL TRAINING\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "from pytorch_lightning.callbacks import (EarlyStopping,LearningRateFinder,ModelCheckpoint)\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_acc', \n",
    "                           min_delta=0.001, \n",
    "                           patience=10,\n",
    "                           mode=\"max\"),\n",
    "             # finds optimal learning rate automatically\n",
    "             LearningRateFinder(min_lr=1e-08, \n",
    "                               max_lr=1, \n",
    "                               num_training_steps=100, \n",
    "                               mode='exponential', \n",
    "                               early_stop_threshold=4.0),\n",
    "            # saves top-K checkpoints based on \"val_acc\" metric\n",
    "            ModelCheckpoint(\n",
    "                save_top_k=5,\n",
    "                monitor=\"val_acc\",\n",
    "                mode=\"max\",\n",
    "                dirpath=\"lightning_logs/{}_1505\".format(dataset_extension),\n",
    "                filename=\"TabularModel-{epoch:02d}-{val_acc:.2f}\",\n",
    "            )]\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=100, \n",
    "                     min_epochs=5,\n",
    "                     callbacks=callbacks,\n",
    "                     logger=True,\n",
    "                     log_every_n_steps=10,\n",
    "                     #enable_checkpointing=True\n",
    "                     accelerator='cpu'\n",
    "                    )\n",
    "\n",
    "trainer.fit(tabular_model, tabular_data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff39575",
   "metadata": {},
   "source": [
    "## TEST SET EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afeae7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n",
    "experiment_name = \"_mbfc_allbias_include_categorical_1505\"\n",
    "# let's set the working directory for saving the reports & ConfusionMatrix\n",
    "if not os.path.exists(\"results/Experiment{}\".format(experiment_name)):\n",
    "    # if the demo_folder directory is not present \n",
    "    # then create it.\n",
    "    os.makedirs(\"results/Experiment{}\".format(experiment_name))\n",
    "    print(\"Made Experiment Folder!\")\n",
    "\n",
    "os.chdir(\"results/Experiment{}/\".format(experiment_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9b2a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "def predict(tabular_model, tabular_data_module):\n",
    "    data_loader = tabular_data_module.test_dataloader()\n",
    "    batch_size = data_loader.batch_size\n",
    "    n_rows = len(tabular_data_module.dataset_test)\n",
    "    \n",
    "    y_true = np.zeros(n_rows, dtype=np.float32)\n",
    "    y_pred = np.zeros((n_rows,5), dtype=np.float32)\n",
    "    indexes = []\n",
    "    with torch.no_grad():\n",
    "        idx = 0\n",
    "        for index, num_batch, cat_batch, label_batch in data_loader:\n",
    "            # get model output\n",
    "            y_output = tabular_model(num_batch, cat_batch)\n",
    "            y_prob = y_output.cpu().numpy()\n",
    "            # map exp function to all outputs\n",
    "            y_prob = list(map(np.exp,y_prob))\n",
    "\n",
    "            # add predictions to output arrays\n",
    "            start_idx = idx\n",
    "            idx += batch_size\n",
    "            end_idx = idx\n",
    "            y_pred[start_idx:end_idx] = y_prob\n",
    "            y_true[start_idx:end_idx] = label_batch.cpu().numpy()\n",
    "            \n",
    "            # append indexes\n",
    "            indexes.extend(index)\n",
    "\n",
    "            if end_idx == n_rows:\n",
    "                break\n",
    "\n",
    "    return y_true, y_pred, indexes\n",
    "\n",
    "def get_AUC_scores(y_test,y_pred):\n",
    "    \"\"\"\n",
    "    Since we need to use OneHotEncoded values for AUC, we'll do that here separately.\n",
    "    We also calculate the AUC per class, and also its average. Then put this in\n",
    "    array and pad it so it fits into the report DataFrame.\n",
    "    \"\"\"\n",
    "    ohe = OneHotEncoder()\n",
    "    out = ohe.fit_transform(y_test.values.reshape(-1, 1)).toarray()\n",
    "    y_test_ohe = pd.DataFrame(out, index=y_test.index)\n",
    "    # get same shape for predictions - note that preds often arrays, not series, so don't need .values\n",
    "    out = ohe.transform(y_pred.reshape(-1, 1)).toarray()\n",
    "    y_pred_ohe = pd.DataFrame(out, index=y_test.index)\n",
    "    \n",
    "    # compute actual scores\n",
    "    AUC_avg = roc_auc_score(y_test_ohe,y_pred_ohe, multi_class='ovr')\n",
    "    AUC_per_class = roc_auc_score(y_test_ohe,y_pred_ohe, average=None,multi_class='ovr')\n",
    "    # add AUC to report DF\n",
    "    AUC_list = np.append(AUC_per_class,AUC_avg)\n",
    "    AUC_list = np.append(AUC_list, [0,0]) # add padding so it fits into DF\n",
    "    \n",
    "    return AUC_list\n",
    "\n",
    "def evaluate_predictions(y_test,y_pred,save=True,model_name=\"\"):\n",
    "    \"\"\"\n",
    "    Note that y_test and y_pred both have to have 1 dimension only here - no probs per class.\n",
    "    \"\"\"\n",
    "    # make list of class names in correct order\n",
    "    class_names = [\"left\",\"left center\",\"least biased\",\"right center\",\"right\"] # in order from MBFC mappings\n",
    "    # make report of predictions\n",
    "    report = pd.DataFrame(classification_report(y_test,y_pred, output_dict=True,\n",
    "                                  target_names=class_names))\n",
    "    # get AUC score & add to report DF\n",
    "    report.loc['AUC',:] = get_AUC_scores(y_test,y_pred)\n",
    "    \n",
    "    # make confusion matrix\n",
    "    disp = ConfusionMatrixDisplay.from_predictions(\n",
    "            y_test,\n",
    "            y_pred,\n",
    "            display_labels = class_names, \n",
    "            xticks_rotation=\"vertical\",\n",
    "            cmap=plt.cm.Blues,\n",
    "        )\n",
    "    disp.ax_.set_title(\"{} Confusion Matrix of Outlet Bias\".format(\"model_name\"))\n",
    "    # print results and timing\n",
    "    print(\"Test accuracy\")\n",
    "    print(f\"{accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "\n",
    "    if save == True:\n",
    "        # save picture and report\n",
    "        save_path_confmx = \"{}_Confusion_Matrix.png\".format(model_name)\n",
    "        save_path_report = \"{}_Report.csv\".format(model_name)\n",
    "        # picture\n",
    "        disp.figure_.savefig(save_path_confmx)\n",
    "        report.to_csv(save_path_report)\n",
    "\n",
    "    return report,disp\n",
    "\n",
    "def eval_model(model,model_name=\"PyTorch\",save=False):\n",
    "    # get true and predicted values (note that y_true here is still prob values per class, not output class)\n",
    "    y_true, y_pred, indexes = predict(model, tabular_data_module)\n",
    "    y_argmax_pred = np.argmax(y_pred,axis=1)\n",
    "    # make y_true nto pd Series with old index intact\n",
    "    y_test = pd.Series(y_true,index=indexes)\n",
    "    # evaluate\n",
    "    report,disp = evaluate_predictions(y_test,y_argmax_pred,save=save,model_name=model_name)\n",
    "    return report,disp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd1bce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(tabular_model,model_name=\"PyTorch\",save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c542ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bd358d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18103d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(tabular_model,\"best_torch_15052023_acc70_auc80.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b2a537",
   "metadata": {},
   "source": [
    "# SHAP - Explaining Model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8f00edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import shap\n",
    "from torch.utils.data import ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7aa4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize JavaScript for visualizing the outputs\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6aaae36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go up one directory\n",
    "os.chdir(\"../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d61699",
   "metadata": {},
   "source": [
    "### Make Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "369d9b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get background from training data\n",
    "background = [torch.tensor(tabular_data_module.train_dataloader().dataset.X_num),\n",
    "              torch.tensor(tabular_data_module.train_dataloader().dataset.X_cat)]\n",
    "# initialise the explainer with the numeric and categorical input\n",
    "explainer = shap.DeepExplainer(tabular_model,background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c6d7bc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_by_index_name(index_name):\n",
    "    \"\"\"\n",
    "    Function for retrieving the data based on outlet name.\n",
    "    \"\"\"\n",
    "    index = tabular_data_module.full_dataloader().dataset.index.tolist() \n",
    "    idx = index.index(index_name) # get index position of\n",
    "    # get info from dataloader\n",
    "    outlet,num_features,cat_features,y_true = tabular_data_module.full_dataloader().dataset[idx]\n",
    "    # convert to proper formats so that PyTorch accepts it, otherwise we get grad issues\n",
    "    num_features = np.array(num_features).astype(np.float32)\n",
    "    cat_features = np.array(cat_features).astype(np.float32)\n",
    "    num_features = torch.tensor(num_features).unsqueeze(0)\n",
    "    cat_features = torch.tensor(cat_features).unsqueeze(0)\n",
    "    \n",
    "    return idx, torch.tensor(num_features), torch.tensor(cat_features),y_true\n",
    "\n",
    "def model_predict(inputs):\n",
    "    num_tensors,cat_tensors = inputs\n",
    "    # get model output\n",
    "    y_output = tabular_model(num_tensors, cat_tensors)\n",
    "    y_pred = y_output.detach().numpy()\n",
    "    # map exp function to all outputs\n",
    "    y_prob = list(map(np.exp,y_pred))\n",
    "\n",
    "    y_argmax_pred = np.argmax(y_prob)\n",
    "    return y_argmax_pred\n",
    "\n",
    "def decision_plot_for_outlet(target_outlet_name,xlim=(-0.5,4.5)):\n",
    "    classes_dict = {0:\"Left\", 1:\"Left lean\",2:\"Least biased\", 3:\"Right lean\",4:\"Right\"}\n",
    "    # get tensors from model\n",
    "    idx, num_tensors, cat_tensors, actual = get_sample_by_index_name(target_outlet_name)\n",
    "\n",
    "    # model prediction was..\n",
    "    prediction = model_predict([num_tensors, cat_tensors])\n",
    "    \n",
    "    print(\"The outlet is: \", target_outlet_name)\n",
    "    print(\"Political leaning is: \", classes_dict[actual])\n",
    "    print(\"Political leaning predicted as: \", classes_dict[prediction])\n",
    "    \n",
    "    # compute the SHAP values using the explainer\n",
    "    shap_values = explainer.shap_values([num_tensors, cat_tensors])\n",
    "    \n",
    "    # plot decision plot for predicted class\n",
    "    class_shap_values = shap_values[prediction][0] # get numeric values\n",
    "\n",
    "    plt.figure()\n",
    "    shap.decision_plot(explainer.expected_value[prediction], \n",
    "                       class_shap_values,\n",
    "                       feature_names = num_cols.tolist(),\n",
    "                       features=np.array(num_tensors),\n",
    "                       xlim=xlim,\n",
    "                       show=0)\n",
    "    plt.title(\"Decision Plot for {}\".format(target_outlet_name.upper()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746045e0",
   "metadata": {},
   "source": [
    "## Get SHAP for Single Outlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa057b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_plot_for_outlet('cnn.com',xlim=(-3.5,4.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b392f428",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_plot_for_outlet(\"breitbart.com\",xlim=(-5,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f5555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_plot_for_outlet(\"theguardian.com\",xlim=(-5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9561e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(shap.decision_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed176707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4209d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c1f45a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2662421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68280361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eaba68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e76ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c293cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74419416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec193464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8914966",
   "metadata": {},
   "source": [
    "# SCRAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d68e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict_for_all_data(subset=\"\"):\n",
    "    if subset == \"train\":\n",
    "        # get the dataset in question\n",
    "        combined_dataset = tabular_data_module.dataset_train\n",
    "        n_rows = len(tabular_data_module.dataset_train)\n",
    "    elif subset == \"val\":\n",
    "        combined_dataset = tabular_data_module.dataset_val\n",
    "        n_rows = len(tabular_data_module.dataset_val)\n",
    "    elif subset == \"test\":\n",
    "        combined_dataset = tabular_data_module.dataset_test\n",
    "        n_rows = len(tabular_data_module.dataset_test)\n",
    "    \n",
    "    else:\n",
    "        # Concatenate the datasets from the three DataLoaders\n",
    "        combined_dataset = ConcatDataset([tabular_data_module.dataset_train, \n",
    "                                          tabular_data_module.dataset_val, \n",
    "                                          tabular_data_module.dataset_test])\n",
    "        n_rows = (len(tabular_data_module.dataset_train) + \n",
    "              len(tabular_data_module.dataset_val) + \n",
    "              len(tabular_data_module.dataset_test))\n",
    "\n",
    "    # Create a new DataLoader for the combined dataset\n",
    "    data_loader = torch.utils.data.DataLoader(combined_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    batch_size = data_loader.batch_size\n",
    "    \n",
    "    y_true = np.zeros(n_rows, dtype=np.float32)\n",
    "    y_pred = np.zeros((n_rows,5), dtype=np.float32)\n",
    "    indexes = []\n",
    "    with torch.no_grad():\n",
    "        idx = 0\n",
    "        for index, num_batch, cat_batch, label_batch in data_loader:\n",
    "            # get model output\n",
    "            y_output = tabular_model(num_batch, cat_batch)\n",
    "            y_prob = y_output.cpu().numpy()\n",
    "            # map exp function to all outputs\n",
    "            y_prob = list(map(np.exp,y_prob))\n",
    "\n",
    "            # add predictions to output arrays\n",
    "            start_idx = idx\n",
    "            idx += batch_size\n",
    "            end_idx = idx\n",
    "            y_pred[start_idx:end_idx] = y_prob\n",
    "            y_true[start_idx:end_idx] = label_batch.cpu().numpy()\n",
    "            \n",
    "            # append indexes\n",
    "            indexes.extend(index)\n",
    "\n",
    "            if end_idx == n_rows:\n",
    "                break\n",
    "\n",
    "    # get argmax (final prediction)\n",
    "    y_argmax_pred = np.argmax(y_pred,axis=1)\n",
    "    \n",
    "    return y_true, y_argmax_pred, indexes\n",
    "\n",
    "def model_predict_SHAP(inputs):\n",
    "    y_true,y_pred,idxs = model_predict_for_all_data()\n",
    "    predictions = pd.DataFrame(y_pred,index=idxs)\n",
    "    return predictions\n",
    "\n",
    "def model_predict_SHAP_train(inputs):\n",
    "    y_true,y_pred,idxs = model_predict_for_all_data(subset=\"train\")\n",
    "    predictions = y_pred.reshape(-1, 1)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c0277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make background dataset (training set) - note that since we're using the KernelExplainer, \n",
    "# we use the pandas dataset!\n",
    "\n",
    "dataset_extension = \"_mbfc_allbias_extrafeatures\"\n",
    "\n",
    "train = pd.read_csv(\"data/train{}.csv\".format(dataset_extension))\n",
    "train.set_index(\"outlet\", inplace=True)\n",
    "\n",
    "val = pd.read_csv(\"data/val{}.csv\".format(dataset_extension))\n",
    "val.set_index(\"outlet\", inplace=True)\n",
    "\n",
    "test = pd.read_csv(\"data/test{}.csv\".format(dataset_extension))\n",
    "test.set_index(\"outlet\", inplace=True)\n",
    "\n",
    "# split each dataset into X and y's\n",
    "X_train = train.drop(\"lean\", axis=1)\n",
    "y_train = train[\"lean\"]\n",
    "\n",
    "X_val = val.drop(\"lean\", axis=1)\n",
    "y_val = val[\"lean\"]\n",
    "\n",
    "X_test = test.drop(\"lean\", axis=1)\n",
    "y_test = test[\"lean\"]\n",
    "\n",
    "# combine all sets so we can visualise the explanation for ANY outlet\n",
    "X = pd.concat([X_train,X_val,X_test])\n",
    "y = pd.concat([y_train,y_val,y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6128c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tabular_data_module.batch_size_train = 2000\n",
    "#outlet,num_tensor,cat_tensor,y_true = next(iter(tabular_data_module.train_dataloader()))\n",
    "#background = (outlet,num_tensor,cat_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d5785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#explainer = shap.DeepExplainer(tabular_model,tabular_data_module.train_dataloader())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
