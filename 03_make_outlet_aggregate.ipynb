{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate articles to outlet-level\n",
    "\n",
    "Take all the articles extracted in the previous step and aggrgate the data so we get one row per news outlet, with relevant features (ex. theme1_AvgTone, theme1_PosTone.... themeX_AvgTone)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import ast\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_on_theme(s):\n",
    "    if isinstance(s,float) or str(s) == \"nan\" or not s:\n",
    "        return []\n",
    "    # make cell into list of themes\n",
    "    return list(ast.literal_eval(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the file and subset the columns to the relevant ones for this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get extras (headers and mdfc)\n",
    "articles = pd.read_csv(\"GDELT_GKG/gkg_csvs/one_year_outlets_eMFD.csv\")\n",
    "#articles = articles.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = ['V2SOURCECOMMONNAME', \n",
    "          'IMGorEMBED', # combo of 3 below\n",
    "          #'V21RELATEDIMAGES', \n",
    "          #'V21SOCIALIMAGEEMBEDS',\n",
    "          #'V21SOCIALVIDEOEMBEDS', \n",
    "          'THEMES_SUBSET', 'PosScore','NegScore', \n",
    "          #'AvgTone',  #not needed since its just sum of pos and neg tone\n",
    "          'Polarity', 'ActRefDens', 'SelfRefDens', 'WordCount',\n",
    "          # get the eMFD features\n",
    "          'care-harm','fairness-cheating', 'loyalty-betrayal', 'authority-subversion',\n",
    "          'sanctity-degradation']\n",
    "\n",
    "articles = articles[subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.shape # (14523991,14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many outlets do we have\n",
    "articles[\"V2SOURCECOMMONNAME\"].unique().shape # 1512 if we do intersect with MBFC, otherwise 21544"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total number of articles per outlet for normalising later (?)\n",
    "total_article_counts = articles.groupby(by=\"V2SOURCECOMMONNAME\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see what kind of tail we're dealing with - seems there's quite a few outlets that haven't published a lot\n",
    "total_article_counts.sort_values(ascending=True).head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compared to top publishers...\n",
    "total_article_counts.sort_values(ascending=True).tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the large amount of data, but some outlets being less active and not publishing often, we may want to exclude outlets that have published less than X articles in the timeframe of the data. This way we avoid mostly uninformative rows.\n",
    "\n",
    "In the 6 months are 181 days, so let's say we'd want outlets that have pusted at least 5 articles per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide whether to exclude outlets with less than X articles in the timeframe\n",
    "filter_out_small_outlets = False\n",
    "\n",
    "min_art_per_day = 2\n",
    "min_articles = min_art_per_day*365\n",
    "\n",
    "# filter out news outlets with fewer than X articles \n",
    "if filter_out_small_outlets:\n",
    "    subset_outlets = total_article_counts[total_article_counts >= min_articles].index\n",
    "    articles = articles[articles[\"V2SOURCECOMMONNAME\"].isin(subset_outlets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many outlets do we have now?\n",
    "articles[\"V2SOURCECOMMONNAME\"].unique().shape # 511 with 6 months, 4349 with all outlets for 1 year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each row in articles where there are multiple themes, we want to split so that there's one theme per row (with one cell keeping track of all related themes in case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get robertson ground truths and separate outlets into either Has-GroundTruth or Not-GroundTruth\n",
    "robertson = pd.read_csv(\"GDELT_GKG/extras/Bias Ratings/robertson.csv\",usecols=[\"domain\",\"score\"])\n",
    "robertson_outlets = robertson.domain.values\n",
    "\n",
    "# start processing\n",
    "start = time.time()\n",
    "# clean column into list format\n",
    "articles[\"THEME\"] = articles[\"THEMES_SUBSET\"].apply(split_on_theme)\n",
    "# add column with list of all themes present in case\n",
    "#articles[\"THEMES_SUBSET\"] = articles[\"THEME\"]\n",
    "# can drop theme_subset for now unless find nice way to utilise later\n",
    "articles.drop(columns=[\"THEMES_SUBSET\"], inplace=True)\n",
    "# expand a row with multiple themes into many rows with unique themes\n",
    "e = articles.explode(\"THEME\").reset_index(drop=True)\n",
    "\n",
    "grouped = e.groupby(by=[\"V2SOURCECOMMONNAME\", \"THEME\"])\n",
    "\n",
    "# for each outler & theme, make a row of sentiment per theme\n",
    "GT_rows = []\n",
    "noGT_rows = [] # outlets which don't have ground truth\n",
    "#i=1\n",
    "for x in tqdm(grouped.groups,desc=\"Splitting into rows per outlet and theme...\"):\n",
    "    outlet, theme = x\n",
    "    t = grouped.get_group(x).aggregate(np.mean)\n",
    "    colnames = [theme + \"_\" + col for col in list(t.index)]\n",
    "    row = pd.DataFrame([t.to_list()], columns=colnames)\n",
    "    # this gets the number of articles for this outlet with this theme\n",
    "    art_num = grouped.get_group(x).aggregate(len)[\"THEME\"]\n",
    "    row[theme + \"_article_count\"] = art_num\n",
    "    row.insert(0, \"outlet\", outlet)\n",
    "    if outlet in robertson_outlets:\n",
    "        #i+=1\n",
    "        GT_rows.append(row)\n",
    "        #if i>50:\n",
    "        #    print(i)\n",
    "    else:\n",
    "        noGT_rows.append(row)\n",
    "\n",
    "print(\"Done splitting!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the rows into outlets in MBFC and those outside of MBFC\n",
    "\"\"\"\n",
    "df_mbfc = pd.read_csv(\"/home/insert_user/GDELT_GKG/extras/Bias Ratings/MBFC_features.csv\")\n",
    "df_mbfc = df_mbfc.rename(columns={'URL': 'outlet', 'Bias Rating': 'lean', \"Latutude\":\"Latitude\"})\n",
    "mbfc_outlets = df_mbfc.outlet.values\n",
    "\n",
    "# split rows into outlets which have or don't have Robertson Ground Truths\n",
    "robertson = pd.read_csv(\"GDELT_GKG/extras/Bias Ratings/robertson.csv\",usecols=[\"domain\",\"score\"])\n",
    "\n",
    "# gather two separate smaller lists of rows for mbfc outlets and all others\n",
    "mbfc_rows = []\n",
    "gdelt_rows = []\n",
    "for row in tqdm(rows):\n",
    "    if row.outlet.values[0] in robertson_outlets:\n",
    "        continue #mbfc_rows.append(row)\n",
    "    elif row.outlet.values[0] in mbfc_outlets:\n",
    "        continue #mbfc_rows.append(row)\n",
    "    else:\n",
    "        gdelt_rows.append(row)\n",
    "\n",
    "print(len(mbfc_rows), len(gdelt_rows))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del rows, articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GDELT Dataset\n",
    "\n",
    "Let's now do the same, but without the MBFC extra information for the rest of the GDELT outlets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_concat(rows,stepsize,save_name):\n",
    "    max_batches = len(rows)\n",
    "    \n",
    "    for batch_start in tqdm(np.arange(0,max_batches,step=stepsize)):\n",
    "    # if file already exists, move on\n",
    "        if os.path.exists(\"/home/insert_user/GDELT_GKG/extras/{}_{}.csv\".format(save_name,int(batch_start))):\n",
    "            print(\"Skipping, file already exists for this batch!\")\n",
    "            continue\n",
    "\n",
    "        batch_end = batch_start + stepsize\n",
    "\n",
    "        if batch_end >= max_batches:\n",
    "            batch_end = max_batches - 1\n",
    "\n",
    "        # concatenate batch\n",
    "        sub_rows = rows[batch_start:batch_end]\n",
    "        sub_df = pd.concat(sub_rows)\n",
    "\n",
    "        # do some processing while we're at it\n",
    "        sub_df.fillna(0, inplace=True) # replace nan's\n",
    "        sub_df.set_index(\"outlet\", inplace=True)\n",
    "        # get aggregate functions per column - mean for all except for article counts\n",
    "        agg_dict = {col:('mean' if not col.endswith(\"_article_count\") else 'sum') \n",
    "                for col in sub_df.columns}\n",
    "        # group by so we get the final rows, apply aggregate function for all cols\n",
    "        res = sub_df.groupby(\"outlet\").aggregate(agg_dict)\n",
    "\n",
    "        # normalise article counts per theme by total article counts per outlet\n",
    "        if \"outlet\" not in res.columns.to_list(): # \n",
    "            res[\"outlet\"] = res.index\n",
    "        res[\"tot_art\"] = res[\"outlet\"].map(total_article_counts)\n",
    "        tot_art_columns = res.columns.str.endswith(\"_article_count\")\n",
    "        res.loc[:,tot_art_columns] = res.loc[:,tot_art_columns].div(res[\"tot_art\"], axis=0)\n",
    "\n",
    "        # save to file\n",
    "        #sub_df.to_csv(\"gdelt_articles_part_{}.csv\".format(batch))\n",
    "        res.to_csv(\"/home/insert_user/GDELT_GKG/extras/{}_{}.csv\".format(save_name,int(batch_start)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_batches = len(GT_rows)\n",
    "stepsize = 100000\n",
    "\n",
    "batch_concat(GT_rows,stepsize=stepsize,save_name=\"robertson_outlets_part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_batches = len(noGT_rows)\n",
    "stepsize = 100000\n",
    "\n",
    "batch_concat(noGT_rows,stepsize=stepsize,save_name=\"gdelt_outlets_2_part\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now combine these concatenated dfs, si"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MBFC Dataset Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat sentiment rows so that we get a line with outlet and theme-sentiment \n",
    "# in one row, populated with 0s when new sentiments added\n",
    "print(\"Concatenating...\")\n",
    "df = pd.concat(mbfc_rows)\n",
    "\n",
    "df.fillna(0, inplace=True) # replace nan's\n",
    "df.set_index(\"outlet\", inplace=True)\n",
    "\n",
    "del colnames,e,grouped,outlet,row,rows,t,theme,x\n",
    "\n",
    "\"\"\"\n",
    "now group by each outlet and aggregate by the average! -> we get a row of \n",
    "each outlet's tones per theme! we want to do the average for all except\n",
    "the counts of how many articles the outlet published featuring a theme\n",
    "\"\"\"\n",
    "\n",
    "print(\"Aggregating...\")\n",
    "\n",
    "agg_dict = {col:('mean' if not col.endswith(\"_article_count\") else 'sum') \n",
    "            for col in df.columns}\n",
    "# group by so we get the final rows\n",
    "res = df.groupby(\"outlet\").aggregate(agg_dict)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"This took {} seconds\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a row per outlet, with each outlet's average tone per theme (and number of articles mentioning that theme).\n",
    "\n",
    "Let's add the features from MBFC to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing MBFC Features data\n",
    "\n",
    "We first need to rename some things..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we need to change something without rerunning all code above\n",
    "#res = pd.read_csv(\"/home/insert_user/GDELT_GKG/outlet_sentiments.csv\")\n",
    "# skip faulty MBFC columns\n",
    "#res = res.iloc[:,:-8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get full dataframe of measures extracted from MBFC\n",
    "df_mbfc = pd.read_csv(\"/home/insert_user/GDELT_GKG/extras/MBFC_features.csv\")\n",
    "# rename columns\n",
    "df_mbfc = df_mbfc.rename(columns={'URL': 'outlet', 'Bias Rating': 'lean', \"Latutude\":\"Latitude\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mbfc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.merge(df_mbfc[[\"outlet\",\"lean\",\"Factuality\",\"PressFreedom\",\"MediaType\",\"Traffic\",\"Credibility\",\"Longitude\",\"Latitude\"]], \n",
    "                on=\"outlet\", how = 'left') # add other categs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since different outlets will greatly vary in how many articles they publish in a day, we want to make sure to normalise the article counts per theme by the total number of articles that the outlet publishes in general - this way we get a ratio of articles per theme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise article counts per theme by total article counts per outlet\n",
    "res[\"tot_art\"] = res[\"outlet\"].map(total_article_counts)\n",
    "tot_art_columns = res.columns.str.endswith(\"_article_count\")\n",
    "res.loc[:,tot_art_columns] = res.loc[:,tot_art_columns].div(res[\"tot_art\"], axis=0)\n",
    "#res.drop(columns=[\"tot_art\"], inplace=True) let's keep this if we want to remove outlets with few articles later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file\n",
    "res.to_csv(\"/home/insert_user/GDELT_GKG/extras/mbfc_outlet_sentiments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.read_csv(\"/home/insert_user/GDELT_GKG/extras/mbfc_outlet_sentiments.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
